{
  "patterns": [
    {
      "id": 1,
      "name": "Two Pointers",
      "concept": "Use two indices moving through an array/string to reduce complexity. Helps avoid nested loops by scanning from both ends or at different speeds.\nThink of it as a way to **scan the input more intelligently** — instead of restarting your search for each element, you use two coordinated \"cursors\" that progressively narrow down the solution space.",
      "when_to_use": [
        "Finding pairs in a sorted array",
        "Checking palindromes",
        "Merging two sorted lists",
        "Sliding window optimizations"
      ],
      "approach": [
        "Initialize two pointers (start/end or fast/slow).",
        "Move them based on conditions.",
        "Check/record results as needed.",
        "Stop when pointers meet or cross."
      ],
      "complexity": {
        "time": "O(n)",
        "space": "O(1)"
      },
      "examples": [
        "167. Two Sum II",
        "125. Valid Palindrome",
        "11. Container With Most Water"
      ],
      "tips": [
        "Works best when input is sorted or order matters.",
        "Use slow/fast pointers for cycle detection.",
        "Don’t forget to handle edge cases (empty/one element)."
      ]
    },
    {
      "id": 2,
      "name": "Merge Intervals",
      "concept": "The **Merge Intervals** pattern deals with overlapping ranges (like meeting times or task schedules). The key idea is to **sort the intervals by start time**, then scan through them while comparing the current interval with the last merged one. If they overlap, merge them; if not, add a new interval to the result. This avoids nested loops and works in O(n log n).",
      "when_to_use": [
        "Interval scheduling",
        "Finding free time slots",
        "Merging ranges in arrays"
      ],
      "approach": [
        "Sort intervals by start time.",
        "Iterate and merge if overlap exists.",
        "Otherwise, add current interval to result."
      ],
      "complexity": {
        "time": "O(n log n) due to sorting",
        "space": "O(n) for output"
      },
      "examples": [
        "56. Merge Intervals",
        "57. Insert Interval",
        "252. Meeting Rooms"
      ],
      "tips": [
        "Sorting first is the key step.",
        "Always compare current interval with the last merged one.",
        "Use greedy merging to keep code clean."
      ]
    },
    {
      "id": 3,
      "name": "Two Heaps",
      "concept": "The **Two Heaps** pattern uses a **min-heap** and a **max-heap** together to efficiently maintain order statistics or dynamically compute medians. One heap stores the smaller half of elements (max-heap), and the other stores the larger half (min-heap). By balancing them after each insertion, you can quickly access the median, k-th largest/smallest, or top elements without sorting the entire dataset each time.",
      "when_to_use": [
        "Find median in a data stream",
        "Balance workloads",
        "Keep track of lower/upper halves"
      ],
      "approach": [
        "Use max-heap for the smaller half, min-heap for the larger half.",
        "Balance sizes after every insertion.",
        "Median is either top of one heap or average of tops."
      ],
      "complexity": {
        "time": "O(log n) per insertion",
        "space": "O(n)"
      },
      "examples": [
        "215. Kth Largest Element in an Array",
        "295. Find Median from Data Stream",
        "703. Kth Largest Element in a Stream",
        "480. Sliding Window Median"
      ],
      "tips": [
        "Balance heaps after every insert.",
        "Keep heaps size difference ≤ 1.",
        "Choose heap type carefully depending on language."
      ]
    },
    {
      "id": 4,
      "name": "Modified Binary Search",
      "concept": "**Modified Binary Search** is an extension of classic binary search used when the problem has additional constraints or unusual structures. Instead of just finding a target, you may need to find: the **first/last occurrence**, the **smallest/largest element meeting a condition**, or search in **rotated/special arrays**. The key is defining the **search space** and **condition function**, then narrowing it down with binary search logic.\n",
      "when_to_use": [
        "Rotated sorted arrays",
        "First/last occurrence search",
        "Minimum/maximum satisfying a constraint (binary search on answer)",
        "Special conditions like “first element ≥ X” or “last element ≤ Y”."
      ],
        "approach": [
            "Define the search space (start/end indices).",
            "Determine the condition to check mid-point.",
            "Decide the condition for moving `left` or `right`",
            "Adjust search space based on condition.",
            "Repeat until search space is exhausted."
        ],
        "complexity": {
          "time": "O(log n)",
          "space": "O(1)"
        },
      "examples": [
        "33. Search in Rotated Sorted Array",
        "34. Find First and Last Position of Element",
        "153. Find Minimum in Rotated Sorted Array"
      ],
      "tips": [
        "Always check which half is sorted.",
        "Visualize search space after each iteration.",
        "Carefully update mid to avoid infinite loops.",
        "Binary search isn’t only for exact matches."
      ]
    },

    {
      "id": 5,
      "name": "Top K Elements",
      "concept": "The **Top K Elements** pattern focuses on efficiently finding the largest or smallest K elements from a dataset without fully sorting it. Common approaches include **heaps** (min-heap for largest K, max-heap for smallest K) or **Quickselect** (partition-based selection). This pattern is useful when K is much smaller than the total number of elements, allowing faster solutions than O(n log n) full sorting.\n",
      "when_to_use": [
        "Find k smallest/largest elements",
        "Frequency-based problems (top K frequent elements)",
        "Streaming data or dynamic updates"
      ],
      "approach": [
        "Use min-heap of size k for largest k.",
        "Use max-heap of size k for smallest k.",
        "Or apply Quickselect for average O(n).",
        "Bucket/counting sort if frequency-based."
      ],
      "complexity": {
        "time": "O(n log k) with heaps, O(n) with quickselect",
        "space": "O(k) or O(n) depending on method"
      },
      "examples": [
        "215. Kth Largest Element in an Array",
        "347. Top K Frequent Elements",
        "703. Kth Largest in a Stream"
      ],
      "tips": [
        "Use a min-heap for “largest K” to efficiently discard smaller elements.",
        "For streaming, heaps are best.",
        "For frequency, use hashmap + bucket sort for efficiency."
      ]
    },
    {
      "id": 6,
      "name": "Backtracking",
      "concept": "Backtracking is a problem-solving technique that explores all possible choices recursively, and abandons paths that violate constraints. It’s like a smart brute force: try a choice, recurse, then undo the choice (backtrack) to explore other options. This is ideal for problems where you need all valid solutions or want to search efficiently under constraints.",
      "when_to_use": [
        "Generating combinations, permutations, or subsets",
        "Constraint satisfaction problems (N-Queens, Sudoku)",
        "Word search or puzzle solving",
        "Problems requiring exploration of all valid paths"
      ],
      "approach": [
        "Make a choice at the current step",
        "Recurse to explore further choices",
        "Undo the choice (backtrack) to try other options",
        "Stop recursion when constraints are violated or solution found"
      ],
      "complexity": {
        "time": "O(k^n) in worst case (explores all possibilities)",
        "space": "O(n) recursion stack + O(n) temporary storage"
      },
      "examples": [
        "46. Permutations",
        "77. Combinations",
        "51. N-Queens",
        "79. Word Search",
        "37. Sudoku Solver"
      ],
      "tips": [
        "Always undo your choice before returning from recursion",
        "Prune early: stop recursion if a path cannot yield a valid solution",
        "Use backtracking with sets or arrays to track used elements",
        "Draw a recursion tree to visualize the exploration process"
      ]
    },
    {
      "id": 7,
      "name": "Dynamic Programming (DP)",
      "concept": "Dynamic Programming solves problems by breaking them into smaller overlapping subproblems and storing their results to avoid redundant computations. It’s ideal for optimization, counting, or path problems where the same subproblem occurs multiple times. DP can be implemented top-down with memoization or bottom-up with tabulation.",
      "when_to_use": [
        "Optimization problems (max/min profit, sum, or score)",
        "Counting problems (ways to reach a target)",
        "Subsequence/subset problems (LIS, LCS, knapsack)",
        "Grid/path problems (unique paths, minimum cost path)"
      ],
      "approach": [
        "Identify the state: what parameters define a subproblem",
        "Define recurrence relation: how current state depends on previous states",
        "Choose memoization (recursive + cache) or tabulation (iterative DP table)",
        "Build up solution using stored results"
      ],
      "complexity": {
        "time": "O(number of states × transitions) — often much faster than brute force",
        "space": "O(number of states) for memoization or DP table"
      },
      "examples": [
        "70. Climbing Stairs (1D DP)",
        "198. House Robber (1D DP)",
        "300. Longest Increasing Subsequence (subsequence DP)",
        "62. Unique Paths (2D DP)",
        "416. Partition Equal Subset Sum (subset DP)"
      ],
      "tips": [
        "Try to recognize overlapping subproblems to justify DP",
        "Decide top-down vs bottom-up based on problem clarity",
        "Optimize space if possible (e.g., use rolling arrays for 1D DP)",
        "Draw small examples to understand state transitions"
      ]
    },
    {
      "id": 8,
      "name": "Tree Breadth-First Search (BFS)",
      "concept": "Breadth-First Search (BFS) is a tree traversal technique that explores nodes level by level, starting from the root. It uses a queue to process nodes in the order they are discovered, making it useful for problems where the solution depends on levels or shortest paths.",
      "when_to_use": [
        "Level order traversal of a tree",
        "Finding the shortest path in an unweighted graph or tree",
        "Problems that require processing nodes layer by layer",
        "Calculating minimum depth, maximum depth, or averages per level"
      ],
      "approach": [
        "Initialize a queue with the root node",
        "While the queue is not empty, process all nodes in the current level",
        "For each node, enqueue its children",
        "Repeat until all levels are processed"
      ],
      "complexity": {
        "time": "O(n) since each node is processed once",
        "space": "O(n) for the queue in the worst case (last level)"
      },
      "examples": [
        "102. Binary Tree Level Order Traversal",
        "107. Binary Tree Level Order Traversal II",
        "111. Minimum Depth of Binary Tree",
        "637. Average of Levels in Binary Tree"
      ],
      "tips": [
        "Use a queue to manage nodes in BFS order",
        "Track the size of the queue to separate levels",
        "Can extend BFS to graphs by marking visited nodes",
        "Great for problems that involve shortest paths or levels"
      ]
    },
    {
      "id": 9,
      "name": "Tree DFS",
      "concept": "Depth-First Search (DFS) is a tree traversal technique that explores as far as possible along one branch before backtracking. DFS can be implemented recursively or iteratively using a stack. It is useful for problems involving full exploration, pathfinding, or subtree calculations.",
      "when_to_use": [
        "Exploring all paths from root to leaf",
        "Checking if a path with a given sum exists",
        "Subtree-related problems (size, height, diameter)",
        "Inorder, Preorder, or Postorder traversals"
      ],
      "approach": [
        "Start from the root node",
        "Recursively or iteratively visit a child before siblings",
        "Use Preorder, Inorder, or Postorder depending on the problem",
        "Backtrack when reaching leaf nodes or null children"
      ],
      "complexity": {
        "time": "O(n) since each node is visited once",
        "space": "O(h) where h is the height of the tree (recursion stack or explicit stack)"
      },
      "examples": [
        "94. Binary Tree Inorder Traversal",
        "104. Maximum Depth of Binary Tree",
        "112. Path Sum",
        "543. Diameter of Binary Tree"
      ],
      "tips": [
        "Use recursion for cleaner DFS code unless stack depth is a concern",
        "Choose traversal order (preorder, inorder, postorder) based on problem",
        "DFS is better than BFS when exploring all possible paths",
        "Consider iterative DFS with a stack to avoid recursion limits"
      ]
    },
    {
      "id": 10,
      "name": "Sliding Window",
      "concept": "Sliding Window is a technique used to reduce nested loops into a single loop by maintaining a window (subarray or substring) that moves over the input. It is especially useful for problems involving subarrays, substrings, or ranges. The window can be of fixed size or variable size depending on the problem.",
      "when_to_use": [
        "Finding maximum/minimum/average in a subarray of fixed size",
        "Finding longest/shortest substring with certain properties",
        "Counting distinct elements in a window",
        "Optimizing subarray or substring problems"
      ],
      "approach": [
        "Initialize left and right pointers to define the window",
        "Expand the right pointer to include more elements",
        "Shrink the left pointer when constraints are violated",
        "Update the result during expansion/shrinking"
      ],
      "complexity": {
        "time": "O(n) since each element is added and removed at most once",
        "space": "O(k) for storing window data (can be O(1) if using counters)"
      },
      "examples": [
        "3. Longest Substring Without Repeating Characters",
        "76. Minimum Window Substring",
        "239. Sliding Window Maximum",
        "567. Permutation in String"
      ],
      "tips": [
        "Use HashMap or frequency array for variable-sized windows",
        "Always check when to shrink the window (important for correctness)",
        "Fixed window size is easier; variable window requires careful condition handling",
        "Draw window movement step-by-step for clarity"
      ]
    },
    {
      "id": 11,
      "name": "Prefix Sum",
      "concept": "Prefix Sum is a technique where you precompute cumulative sums (or counts) up to each index, enabling fast range queries and efficient calculations over subarrays or substrings. Instead of recalculating sums repeatedly, you reuse precomputed values for O(1) query time.",
      "when_to_use": [
        "Range sum queries in arrays or matrices",
        "Checking subarrays with target sum",
        "Counting prefix-based patterns (like number of subarrays with even/odd sum)",
        "2D grid problems involving submatrix sums"
      ],
      "approach": [
        "Precompute prefix array where prefix[i] = sum of elements from start to i",
        "To find sum of subarray (l..r), use prefix[r] - prefix[l-1]",
        "For 2D, build prefix matrix where prefix[i][j] = sum of elements in rectangle (0,0) to (i,j)",
        "Use prefix differences to answer queries in O(1)"
      ],
      "complexity": {
        "time": "O(n) preprocessing + O(1) per query",
        "space": "O(n) for 1D prefix, O(n×m) for 2D prefix matrix"
      },
      "examples": [
        "303. Range Sum Query - Immutable",
        "304. Range Sum Query 2D - Immutable",
        "560. Subarray Sum Equals K",
        "2389. Longest Subsequence With Limited Sum"
      ],
      "tips": [
        "Initialize prefix[0] = 0 for easier calculations",
        "Use HashMap with running sum for subarray sum problems",
        "Be careful with off-by-one indexing when computing prefix differences",
        "Extend the idea to counts, XOR, or other operations, not just sums"
      ]
    },
    {
      "id": 12,
      "name": "Greedy",
      "concept": "Greedy is a problem-solving strategy that makes the locally optimal choice at each step with the hope of reaching a globally optimal solution. It works when local decisions lead to the overall best answer. Greedy is often simpler and faster than dynamic programming but only applies when the problem has the greedy-choice property and optimal substructure.",
      "when_to_use": [
        "Optimization problems (minimum/maximum result)",
        "Interval scheduling and activity selection",
        "Resource allocation (min coins, min arrows, etc.)",
        "Graph algorithms like Kruskal’s or Prim’s MST, Dijkstra’s shortest path"
      ],
      "approach": [
        "Identify the decision to make at each step",
        "Define a rule for the locally optimal choice",
        "Iteratively apply the choice while maintaining constraints",
        "Prove or check correctness (greedy-choice property and optimal substructure)"
      ],
      "complexity": {
        "time": "Often O(n log n) due to sorting or O(n) with a simple greedy pass",
        "space": "Usually O(1) or O(n) depending on data structures used"
      },
      "examples": [
        "55. Jump Game",
        "45. Jump Game II",
        "435. Non-overlapping Intervals",
        "452. Minimum Number of Arrows to Burst Balloons",
        "1005. Maximize Sum Of Array After K Negations"
      ],
      "tips": [
        "Check if greedy works: does a local best always lead to a global best?",
        "Greedy is often combined with sorting (e.g., intervals, activities)",
        "If greedy doesn’t work, try dynamic programming",
        "Draw small examples to test correctness of your greedy strategy"
      ]
    },
    {
      "id": 13,
      "name": "Bit Manipulation",
      "concept": "Bit Manipulation involves using bitwise operations (AND, OR, XOR, NOT, shifts) to solve problems efficiently. It leverages binary representation of numbers to optimize calculations, often reducing time and space complexity compared to conventional approaches.",
      "when_to_use": [
        "Finding unique or missing elements in arrays",
        "Checking if a number is even/odd, power of two, or has certain bits set",
        "Subsets generation using bitmasks",
        "Optimizing arithmetic or state representation in problems"
      ],
      "approach": [
        "Use XOR for finding unique elements (since x ^ x = 0, x ^ 0 = x)",
        "Use bit shifts for fast multiplication/division by 2",
        "Use bitmasks to represent subsets or states compactly",
        "Apply AND/OR for checking or setting specific bits"
      ],
      "complexity": {
        "time": "O(1) per bit operation, often O(n) overall",
        "space": "O(1) for simple operations, O(2^n) if generating subsets"
      },
      "examples": [
        "136. Single Number",
        "190. Reverse Bits",
        "191. Number of 1 Bits",
        "338. Counting Bits",
        "78. Subsets (using bitmasking)"
      ],
      "tips": [
        "Remember key identities: x ^ x = 0, x ^ 0 = x",
        "Use n & (n-1) trick to remove the lowest set bit",
        "Check if n is power of two using n & (n-1) == 0",
        "Think in terms of binary representation for subset or state problems"
      ]
    }
  ]
}

